#!/usr/bin/env python3
"""
Cleaned & patched 2UP API scraper.

Usage:
    python scrape_2up_api_only.py --out out.json --max 20
        [--hours 48] [--page-size 50] [--start-page 1]
        [--cookies "name=value; ..."] [--sign SIGN] [--ts TIMESTAMP]
        [--verbose]

Environment fallbacks:
    TWOUP_COOKIES, TWOUP_SIGN, TWOUP_TS

Notes:
    - This script calls POST https://2up.io/api/sportProtal/web/event/date/list?eventDateList
    - Provide cookies + X-Request-Sign + X-Request-Timestamp if the server requires them.
"""
from __future__ import annotations

import argparse
import hashlib
import json
import os
import sys
import time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import requests
from requests.adapters import HTTPAdapter, Retry

API_URL = "https://2up.io/api/sportProtal/web/event/date/list?eventDateList"
REFERER_URL = "https://2up.io/pt/sports/home?section=upcoming&sport=soccer"
BOOKMAKER_KEY = "2up"

# env / hardcoded defaults
HARDCODE_COOKIES = os.getenv("TWOUP_COOKIES", "")
HARDCODE_SIGN = os.getenv("TWOUP_SIGN", "")
HARDCODE_TS = os.getenv("TWOUP_TS", "")

# emojis & small helpers
EMO_OK = "‚úÖ"
EMO_RUN = "üöÄ"
EMO_WARN = "‚ö†Ô∏è"
EMO_ERR = "‚ùå"
EMO_PAGE = "üìÑ"
EMO_MATCH = "üèüÔ∏è"
EMO_SAVE = "üíæ"


def info(msg: str) -> None:
    print(f"{EMO_RUN} {msg}")


def ok(msg: str) -> None:
    print(f"{EMO_OK} {msg}")


def warn(msg: str) -> None:
    print(f"{EMO_WARN} {msg}")


def err(msg: str) -> None:
    print(f"{EMO_ERR} {msg}")


def format_decimal_str(value: Any) -> Optional[str]:
    """Normalize numeric/decimal strings to a simple 3-decimal string or None."""
    try:
        if value is None:
            return None
        if isinstance(value, str):
            value = value.replace(",", ".")
        val = float(value)
        return f"{val:.3f}"
    except Exception:
        return None


def compute_match_id(home: str, away: str, date_iso: Optional[str]) -> int:
    key = f"{home}__{away}__{date_iso or ''}"
    digest = hashlib.sha256(key.encode("utf-8")).hexdigest()
    return int(digest[:10], 16)


def build_item_skeleton_from_api(event_node: Dict[str, Any]) -> Dict[str, Any]:
    home = str(event_node.get("homeTeamName") or "")
    away = str(event_node.get("awayTeamName") or "")
    event_time = event_node.get("eventTime")
    date_iso = None
    try:
        if isinstance(event_time, str) and event_time.isdigit():
            event_time = int(event_time)
        if isinstance(event_time, (int, float)):
            dt = datetime.fromtimestamp(int(event_time) / 1000.0, tz=timezone.utc)
            date_iso = dt.isoformat().replace("+00:00", "Z")
    except Exception:
        date_iso = None

    league = str(event_node.get("leagueName") or "")
    sport_slug = str(event_node.get("sportUrl") or "soccer")
    region = str(event_node.get("regionUrl") or "")
    league_url = str(event_node.get("leagueUrl") or "")
    event_url = str(event_node.get("eventUrl") or "")
    pretty_url = (
        f"https://2up.io/pt/sports/{sport_slug}/{region}/{league_url}/{event_url}"
        if event_url
        else ""
    )
    event_id_str = str(event_node.get("eventId") or "")
    if event_id_str:
        match_id = int(hashlib.sha256(event_id_str.encode("utf-8")).hexdigest()[:10], 16)
    else:
        match_id = compute_match_id(home, away, date_iso)
    return {
        "id": match_id,
        "home": home,
        "away": away,
        "date": date_iso or "",
        "sport": {"name": "Football", "slug": "soccer"},
        "league": {"name": league, "slug": league.lower().replace(" ", "-") if league else ""},
        "urls": {BOOKMAKER_KEY: pretty_url},
        "bookmakers": {BOOKMAKER_KEY: []},
        "status": "pending",
    }


def extract_markets_from_api(event_node: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract ML (moneyline), Totals, and Spread (Asian Handicap) markets into a normalized structure.
    Keep the same core logic you provided, but a bit cleaned up.
    """
    now_iso = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    markets_out: List[Dict[str, Any]] = []
    all_markets = event_node.get("markets") or []
    ml_done = False
    totals_lines: Dict[float, Dict[str, Any]] = {}
    handicap_lines: Dict[float, Dict[str, Any]] = {}

    # Optional: quick event label for logs
    event_name = event_node.get("eventName") or f"{event_node.get('homeTeamName','')} vs {event_node.get('awayTeamName','')}"

    for m in all_markets:
        name = str(m.get("name") or "").lower()
        mtid = str(m.get("marketTypeId") or "").lower()
        selections = m.get("selections") or []

        # FT 1X2 (Moneyline)
        if not ml_done and (name == "ft 1x2" or mtid == "ml0"):
            home_p = draw_p = away_p = None
            for s in selections:
                outcome = str(s.get("outcomeType") or "").lower()
                dec = s.get("trueOdds") or (s.get("displayOdds") or {}).get("Decimal")
                price = format_decimal_str(dec)
                if not price:
                    continue
                if outcome == "home":
                    home_p = price
                elif outcome in ("tie", "draw"):
                    draw_p = price
                elif outcome == "away":
                    away_p = price
            if home_p and draw_p and away_p:
                markets_out.append({"name": "ML", "updatedAt": now_iso, "odds": [{"home": home_p, "draw": draw_p, "away": away_p}]})
                ml_done = True
            continue

        # FT O/U (Totals)
        if (name == "ft o/u" or mtid == "ou0"):
            for s in selections:
                ou = str(s.get("outcomeType") or s.get("name") or "").lower()
                points = s.get("points")
                try:
                    if isinstance(points, str):
                        points = float(points.replace(",", "."))
                    elif isinstance(points, (int, float)):
                        points = float(points)
                    else:
                        continue
                except Exception:
                    continue
                dec = s.get("trueOdds") or (s.get("displayOdds") or {}).get("Decimal")
                price = format_decimal_str(dec)
                if not price:
                    continue
                rec = totals_lines.setdefault(points, {"hdp": points, "over": None, "under": None})
                if "over" in ou:
                    rec["over"] = price
                elif "under" in ou:
                    rec["under"] = price
            continue

        # FT Asian Handicap (Spread)
        if (name == "ft asian handicap" or mtid == "hc0"):
            home_pts = None
            home_price = None
            away_pts = None
            away_price = None
            for s in selections:
                outcome = str(s.get("outcomeType") or "").lower()
                points = s.get("points")
                try:
                    if isinstance(points, str):
                        # normalize unicode minus and commas
                        pts_str = points.replace(",", ".").replace("‚àí", "-")
                        points = float(pts_str)
                    elif isinstance(points, (int, float)):
                        points = float(points)
                    else:
                        continue
                except Exception:
                    continue
                dec = s.get("trueOdds") or (s.get("displayOdds") or {}).get("Decimal")
                price = format_decimal_str(dec)
                if not price:
                    continue
                if outcome == "home":
                    home_pts = points
                    home_price = price
                elif outcome == "away":
                    away_pts = points
                    away_price = price
            if home_pts is not None and away_pts is not None and home_price and away_price:
                handicap_lines[float(home_pts)] = {"hdp": float(home_pts), "home": home_price, "away": away_price}
            continue

    # Emit collected markets
    totals_out = [v for v in totals_lines.values() if v.get("over") and v.get("under")]
    if totals_out:
        totals_out.sort(key=lambda x: x["hdp"])
        markets_out.append({"name": "Totals", "updatedAt": now_iso, "odds": totals_out})
    if handicap_lines:
        lines = list(handicap_lines.values())
        lines.sort(key=lambda x: x["hdp"])
        markets_out.append({"name": "Spread", "updatedAt": now_iso, "odds": lines})

    # Log a small per-match summary
    try:
        ml_present = any(m.get("name") == "ML" for m in markets_out)
        totals_cnt = next((len(m.get("odds", [])) for m in markets_out if m.get("name") == "Totals"), 0)
        spread_cnt = next((len(m.get("odds", [])) for m in markets_out if m.get("name") == "Spread"), 0)
        ok(f"{EMO_MATCH} {event_name}: ML={'yes' if ml_present else 'no'} | Totals={totals_cnt} | Spread={spread_cnt}")
    except Exception:
        pass

    return markets_out


def make_session_with_retries(retries: int = 3, backoff: float = 0.3) -> requests.Session:
    s = requests.Session()
    retries_cfg = Retry(
        total=retries,
        backoff_factor=backoff,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
    )
    s.mount("https://", HTTPAdapter(max_retries=retries_cfg))
    s.mount("http://", HTTPAdapter(max_retries=retries_cfg))
    return s


def build_headers(cookies: str, x_sign: str, x_ts: str) -> Dict[str, str]:
    # Apply overrides, falling back to hardcoded env values
    if not cookies and HARDCODE_COOKIES:
        cookies = HARDCODE_COOKIES
    if not x_sign and HARDCODE_SIGN:
        x_sign = HARDCODE_SIGN
    if not x_ts and HARDCODE_TS:
        x_ts = HARDCODE_TS

    headers: Dict[str, str] = {
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "pt-PT,pt;q=0.9,en;q=0.8",
        "Cache-Control": "no-store, no-cache, must-revalidate, max-age=0",
        "Content-Type": "application/json",
        # Some custom headers observed in the browser traffic:
        "Lang": "pt",
        "Odds": "Decimal",
        "Origin": "https://2up.io",
        "Priority": "u=1, i",
        "Referer": REFERER_URL,
        "Sec-Ch-Ua": "\"Chromium\";v=\"142\", \"Google Chrome\";v=\"142\", \"Not A Brand\";v=\"99\"",
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": "\"Windows\"",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-origin",
        "Socketclientid": "a1a51a18-d1de-44d1-8a36-ff739020987d",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
        "Zoneoffset": "0",
    }
    # Only set sensitive headers if values are provided
    if cookies:
        headers["Cookie"] = cookies
    if x_sign:
        headers["X-Request-Sign"] = x_sign
    if x_ts:
        headers["X-Request-Timestamp"] = x_ts
    return headers


def scrape_api_only(
    output_path: str,
    max_matches: int = 20,
    hours_ahead: int = 48,
    page_size: int = 50,
    page_num_start: int = 1,
    cookies: str = "",
    x_sign: str = "",
    x_ts: str = "",
    verbose: bool = False,
    exhaust: bool = True,
) -> List[Dict[str, Any]]:
    now = datetime.now(timezone.utc)
    window_ms = int(hours_ahead * 3600 * 1000)
    start_ms = int(now.timestamp() * 1000)
    end_ms = start_ms + window_ms

    results: List[Dict[str, Any]] = []
    seen_event_ids: set[str] = set()

    session = make_session_with_retries()
    headers = build_headers(cookies, x_sign, x_ts)
    if verbose:
        info(f"{EMO_PAGE} Headers preview:\n{json.dumps(headers, ensure_ascii=False, indent=2)}")
    session.headers.update(headers)

    if headers.get("Cookie"):
        info("Using cookies header üç™")
    if headers.get("X-Request-Sign") and headers.get("X-Request-Timestamp"):
        info("Using X-Request-Sign and X-Request-Timestamp üîê")

    window_idx = 0
    # Iterate windows until empty or until max reached (unless exhaust disables the cap)
    while True:
        window_idx += 1
        human_start = datetime.fromtimestamp(start_ms / 1000, tz=timezone.utc).isoformat().replace("+00:00", "Z")
        human_end = datetime.fromtimestamp(end_ms / 1000, tz=timezone.utc).isoformat().replace("+00:00", "Z")
        info(f"Window #{window_idx} {human_start} ‚Üí {human_end}")

        page = page_num_start
        got_any_in_window = False
        while True:
            # Respect max unless exhaust wants to continue regardless
            if not exhaust and len(results) >= max_matches:
                break

            payload = {
                "isLive": 0,
                "pageType": 5,
                "sportUrl": "soccer",
                "regionUrl": "",
                "leagueUrl": "",
                "startTime": start_ms,
                "endTime": end_ms,
                "pageSize": min(page_size, max_matches if not exhaust else page_size),
                "pageNum": page,
            }
            info(f"Fetching page {page} ‚Ä¶ {EMO_PAGE}")
            if verbose:
                print(f"{EMO_PAGE} Payload: {json.dumps(payload)}")

            try:
                resp = session.post(API_URL, json=payload, timeout=30)
            except Exception as e:
                err(f"Request error: {e}")
                raise

            print(f"{('‚úÖ' if resp.status_code == 200 else '‚ùå')} HTTP {resp.status_code}")
            if resp.status_code != 200:
                err(f"API status {resp.status_code}: {resp.text[:400]}")
                warn("Tip: Pass your current browser cookies (and X-Request-Sign/Timestamp if present) via --cookies --sign --ts")
                raise RuntimeError(f"API status {resp.status_code}")

            try:
                data = resp.json()
            except Exception as e:
                err(f"Failed to parse JSON response: {e}")
                raise

            if not isinstance(data, dict) or str(data.get("code")) != "200":
                err(f"API envelope error: {str(data)[:400]}")
                warn("Server returned code != 200. This endpoint may require signed headers.")
                warn('Try: python scrape_2up_api_only.py --out out.json --cookies="<paste cookies>" --sign="<X-Request-Sign>" --ts="<X-Request-Timestamp>"')
                raise RuntimeError("API envelope error")

            items = (((data.get("data") or {}).get("items")) or [])
            ok(f"Received {len(items)} items on page {page}")

            if not items:
                break

            # Collect unique events
            for ev in items:
                ev_id = str(ev.get("eventId") or "")
                if ev_id and ev_id in seen_event_ids:
                    continue
                if not exhaust and len(results) >= max_matches:
                    break
                item = build_item_skeleton_from_api(ev)
                markets = extract_markets_from_api(ev)
                if markets:
                    item["bookmakers"][BOOKMAKER_KEY] = markets
                results.append(item)
                if ev_id:
                    seen_event_ids.add(ev_id)
                got_any_in_window = True

            total_pages = int((data.get("data") or {}).get("totalPages") or 1)
            page_now = int((data.get("data") or {}).get("page") or page)
            if page_now >= total_pages:
                break
            page += 1

        # Stop if reached cap (when not exhausting) or nothing returned in this window
        if (not exhaust and len(results) >= max_matches) or not got_any_in_window:
            break
        # Advance window
        start_ms = end_ms + 1
        end_ms = start_ms + window_ms

    # Trim to max if needed
    if not exhaust:
        results = results[:max_matches]
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    ok(f"{EMO_SAVE} Wrote {len(results)} matches to {output_path}")
    return results


def parse_args():
    p = argparse.ArgumentParser(description="Scrape upcoming soccer events from 2up.io (API-only).")
    p.add_argument("--out", "-o", default="output_2up.json", help="Output JSON file path")
    p.add_argument("--max", "-m", type=int, default=20, help="Maximum number of matches to fetch")
    p.add_argument("--hours", type=int, default=48, help="Hours ahead window to fetch")
    p.add_argument("--page-size", type=int, default=50, help="Page size for API requests")
    p.add_argument("--start-page", type=int, default=1, help="Start page number")
    p.add_argument("--cookies", type=str, default="", help='Cookies string (e.g. "name=value; name2=value2")')
    p.add_argument("--sign", type=str, default="", help="X-Request-Sign header value")
    p.add_argument("--ts", type=str, default="", help="X-Request-Timestamp header value")
    p.add_argument("--verbose", action="store_true", help="Verbose output")
    p.add_argument("--exhaust", action="store_true", help="Keep advancing time windows until no items are returned")
    return p.parse_args()


def main():
    args = parse_args()
    try:
        info(f"Starting API-only scrape ‚Üí out={args.out}, max={args.max}")
        scrape_api_only(
            output_path=args.out,
            max_matches=args.max,
            hours_ahead=args.hours,
            page_size=args.page_size,
            page_num_start=args.start_page,
            cookies=args.cookies,
            x_sign=args.sign,
            x_ts=args.ts,
            verbose=args.verbose,
            exhaust=args.exhaust,
        )
    except Exception as e:
        err(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
